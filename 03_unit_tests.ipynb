{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT19T+sZQ+dsAYQ6WGGqrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/rag-workflow/blob/main/03_unit_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TugtxdWdv1Mg",
        "outputId": "fb2f17ec-0315-4e67-8e5f-18ba06f09203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rag-workflow'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 44 (delta 15), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 69.86 KiB | 687.00 KiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/wesslen/rag-workflow.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['API_KEY'] = userdata.get('API_KEY')\n",
        "os.environ['BASE_URL'] = userdata.get('BASE_URL')"
      ],
      "metadata": {
        "id": "eEQYj5F5zJLD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class TestCaseGenerator:\n",
        "    def __init__(self, eval_results_path: str):\n",
        "        self.results_path = eval_results_path\n",
        "        self.results = self._load_results()\n",
        "        self.hit_data = self._process_hit_data()\n",
        "\n",
        "    def _load_results(self) -> Dict[str, Any]:\n",
        "        with open(self.results_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _process_hit_data(self) -> pd.DataFrame:\n",
        "        return pd.DataFrame({\n",
        "            'query': self.results['queries'],\n",
        "            'chunk': self.results['original_chunks'],\n",
        "            'hit': self.results['hit_at_k'],\n",
        "            'retrieval_time': self.results['retrieval_times']\n",
        "        })\n",
        "\n",
        "    def _generate_test_case(self, idx: int, row: pd.Series) -> str:\n",
        "        query = row['query'].replace(\"'\", \"\\\\'\").replace(\"\\n\", \" \")\n",
        "        expected_chunk = row['chunk'].replace(\"'\", \"\\\\'\").replace(\"\\n\", \" \")\n",
        "        avg_time = row['retrieval_time']\n",
        "\n",
        "        return f'''\n",
        "def test_retrieval_{idx}(rag_engine):\n",
        "    \"\"\"Test retrieval for query: {query}\"\"\"\n",
        "    query = '{query}'\n",
        "    expected_chunk = '{expected_chunk}'\n",
        "\n",
        "    # Time the retrieval\n",
        "    start_time = time.time()\n",
        "    results = rag_engine.retrieve(query)\n",
        "    retrieval_time = time.time() - start_time\n",
        "\n",
        "    # Convert results to text for comparison\n",
        "    retrieved_texts = [node.text for node in results]\n",
        "\n",
        "    # Normalize texts for comparison\n",
        "    normalized_expected = normalize_text(expected_chunk)\n",
        "    normalized_retrieved = [normalize_text(text) for text in retrieved_texts]\n",
        "\n",
        "    # Assert the expected chunk is in the results\n",
        "    assert any(normalized_expected in text for text in normalized_retrieved), \\\\\n",
        "        f\"Expected chunk not found in retrieval results\"\n",
        "\n",
        "    # Assert retrieval time is within acceptable range (2x baseline)\n",
        "    assert retrieval_time < {avg_time * 2}, \\\\\n",
        "        f\"Retrieval took too long: {{retrieval_time:.2f}}s > {avg_time * 2:.2f}s\"\n",
        "'''\n",
        "\n",
        "    def _generate_test_file_content(self, test_cases: List[str]) -> str:\n",
        "        header = f'''\"\"\"\n",
        "Automatically generated test cases for RAG retrieval.\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "import pytest\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Normalize text for comparison.\"\"\"\n",
        "    return ' '.join(text.lower().split())\n",
        "\n",
        "# Performance test constants\n",
        "TEST_QUERIES = [\n",
        "    \"What are the requirements for medical licenses?\",\n",
        "    \"How should employees handle confidential information?\",\n",
        "    \"What are the policies on conflicts of interest?\"\n",
        "]\n",
        "\n",
        "def test_retrieval_performance(rag_engine):\n",
        "    \"\"\"Test overall retrieval performance.\"\"\"\n",
        "    times = []\n",
        "    for query in TEST_QUERIES:\n",
        "        start_time = time.time()\n",
        "        results = rag_engine.retrieve(query)\n",
        "        retrieval_time = time.time() - start_time\n",
        "        times.append(retrieval_time)\n",
        "\n",
        "        assert len(results) > 0, f\"No results returned for query: {{query}}\"\n",
        "\n",
        "    avg_time = sum(times) / len(times)\n",
        "    assert avg_time < 2.0, f\"Average retrieval time too high: {{avg_time:.2f}}s\"\n",
        "\n",
        "{test_cases}\n",
        "'''\n",
        "        return header\n",
        "\n",
        "    def generate_test_file(self, output_path: str = 'test_rag_retrieval.py'):\n",
        "        successful_cases = self.hit_data[self.hit_data['hit'] == 1]\n",
        "        test_cases = []\n",
        "\n",
        "        for idx, row in successful_cases.iterrows():\n",
        "            test_case = self._generate_test_case(idx, row)\n",
        "            test_cases.append(test_case)\n",
        "\n",
        "        test_file_content = self._generate_test_file_content('\\n'.join(test_cases))\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(test_file_content)\n",
        "\n",
        "        print(f\"Generated {len(test_cases)} test cases in {output_path}\")\n",
        "        return test_file_content\n",
        "\n",
        "\n",
        "def analyze_test_coverage(test_generator: TestCaseGenerator) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze test coverage statistics.\"\"\"\n",
        "    total_queries = len(test_generator.hit_data)\n",
        "    successful_queries = len(test_generator.hit_data[test_generator.hit_data['hit'] == 1])\n",
        "    failed_queries = total_queries - successful_queries\n",
        "\n",
        "    coverage_stats = {\n",
        "        'total_queries': total_queries,\n",
        "        'successful_queries': successful_queries,\n",
        "        'failed_queries': failed_queries,\n",
        "        'coverage_percentage': (successful_queries / total_queries) * 100 if total_queries > 0 else 0,\n",
        "        'avg_retrieval_time': test_generator.hit_data['retrieval_time'].mean(),\n",
        "        'max_retrieval_time': test_generator.hit_data['retrieval_time'].max(),\n",
        "        'min_retrieval_time': test_generator.hit_data['retrieval_time'].min(),\n",
        "        'std_retrieval_time': test_generator.hit_data['retrieval_time'].std()\n",
        "    }\n",
        "\n",
        "    print(\"\\nTest Coverage Analysis:\")\n",
        "    print(f\"Total Queries: {coverage_stats['total_queries']}\")\n",
        "    print(f\"Successful Queries: {coverage_stats['successful_queries']}\")\n",
        "    print(f\"Failed Queries: {coverage_stats['failed_queries']}\")\n",
        "    print(f\"Coverage Percentage: {coverage_stats['coverage_percentage']:.2f}%\")\n",
        "    print(f\"Average Retrieval Time: {coverage_stats['avg_retrieval_time']:.3f}s\")\n",
        "    print(f\"Max Retrieval Time: {coverage_stats['max_retrieval_time']:.3f}s\")\n",
        "    print(f\"Min Retrieval Time: {coverage_stats['min_retrieval_time']:.3f}s\")\n",
        "    print(f\"Std Dev Retrieval Time: {coverage_stats['std_retrieval_time']:.3f}s\")\n",
        "\n",
        "    return coverage_stats\n",
        "\n",
        "def generate_performance_report(coverage_stats: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"Generate a detailed performance report.\"\"\"\n",
        "    performance_data = {\n",
        "        'Metric': [\n",
        "            'Average Retrieval Time',\n",
        "            'Maximum Retrieval Time',\n",
        "            'Minimum Retrieval Time',\n",
        "            'Standard Deviation',\n",
        "            'Coverage Percentage'\n",
        "        ],\n",
        "        'Value': [\n",
        "            f\"{coverage_stats['avg_retrieval_time']:.3f}s\",\n",
        "            f\"{coverage_stats['max_retrieval_time']:.3f}s\",\n",
        "            f\"{coverage_stats['min_retrieval_time']:.3f}s\",\n",
        "            f\"{coverage_stats['std_retrieval_time']:.3f}s\",\n",
        "            f\"{coverage_stats['coverage_percentage']:.2f}%\"\n",
        "        ]\n",
        "    }\n",
        "    return pd.DataFrame(performance_data)"
      ],
      "metadata": {
        "id": "gzUmEj-Fz4gJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TestCaseGenerator('/content/rag-workflow/data/text.eval_results.json')\n",
        "test_content = generator.generate_test_file()\n",
        "print(\"Tests generated successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF7Z-ZTn0DUq",
        "outputId": "31eb93b8-fb0a-4b0a-8e67-f023fa804ae7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 174 test cases in test_rag_retrieval.py\n",
            "Tests generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze coverage\n",
        "coverage_stats = analyze_test_coverage(generator)\n",
        "\n",
        "# Generate and display performance report\n",
        "performance_df = generate_performance_report(coverage_stats)\n",
        "display(HTML(performance_df.to_html(index=False)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ax4rZnEEwuBX",
        "outputId": "44294ed7-3956-4afd-bfd6-6dd82993b777"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Coverage Analysis:\n",
            "Total Queries: 220\n",
            "Successful Queries: 174\n",
            "Failed Queries: 46\n",
            "Coverage Percentage: 79.09%\n",
            "Average Retrieval Time: 0.845s\n",
            "Max Retrieval Time: 1.055s\n",
            "Min Retrieval Time: 0.799s\n",
            "Std Dev Retrieval Time: 0.035s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Metric</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Average Retrieval Time</td>\n",
              "      <td>0.845s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Maximum Retrieval Time</td>\n",
              "      <td>1.055s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Minimum Retrieval Time</td>\n",
              "      <td>0.799s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Standard Deviation</td>\n",
              "      <td>0.035s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Coverage Percentage</td>\n",
              "      <td>79.09%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas to extend tests and RAG evaluation workflow\n",
        "\n",
        "Based on ideas from Jason Liu's [The RAG Playbook](https://jxnl.co/writing/2024/08/19/rag-flywheel/) and [Systematically Improving your RAG](https://jxnl.co/writing/2024/05/22/systematically-improving-your-rag/).\n",
        "\n",
        "## Synthetic Data Generation\n",
        "\n",
        "Generate more synthetic queries using these patterns from the article:\n",
        "- Questions about each text chunk\n",
        "- System limitations queries\n",
        "- Edge case queries\n",
        "\n",
        "Example implementation:\n",
        "```python\n",
        "def generate_synthetic_queries(chunk):\n",
        "    queries = [\n",
        "        f\"What does the article say about {chunk[:50]}?\", # Direct\n",
        "        f\"Can you explain the concept of {chunk[:50]}?\",  # Conceptual\n",
        "        f\"What are examples of {chunk[:50]}?\"  # Examples\n",
        "    ]\n",
        "    return queries\n",
        "```\n",
        "\n",
        "## Experiment Ideas\n",
        "\n",
        "### 1. Try different chunking Strategies\n",
        "```python\n",
        "# Recursive chunking\n",
        "from llama_index.node_parsers import RecursiveNodeParser\n",
        "parser = RecursiveNodeParser(chunk_sizes=[2048, 512, 128])\n",
        "\n",
        "# Semantic chunking - group by topic similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "```\n",
        "\n",
        "### 2. Embedding Models\n",
        "Try:\n",
        "- all-MiniLM-L6-v2 (fast baseline)\n",
        "- all-mpnet-base-v2 (better quality)\n",
        "- BGE-M3 (optimized for RAG)\n",
        "\n",
        "### 3. Topic Modeling & Clustering\n",
        "\n",
        "Use [BERTopic](https://maartengr.github.io/BERTopic/index.html)\n",
        "\n",
        "```python\n",
        "# Using BERTopic\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(documents)\n",
        "\n",
        "# Key topics from article:\n",
        "# - RAG System Implementation\n",
        "# - Metrics & Evaluation\n",
        "# - Real-world Data Analysis\n",
        "# - Continuous Improvement\n",
        "```\n",
        "\n",
        "### 4. Evaluation Pipeline\n",
        "```python\n",
        "def evaluate_retrieval(index, test_cases):\n",
        "    metrics = {\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'latency': []\n",
        "    }\n",
        "    \n",
        "    for query, expected in test_cases:\n",
        "        results = index.as_query_engine().query(query)\n",
        "        # Add precision/recall calculation\n",
        "        # Add latency tracking\n",
        "        \n",
        "    return metrics\n",
        "```\n",
        "\n",
        "## Experimental Matrix\n",
        "\n",
        "Test combinations of:\n",
        "- Chunk sizes: [128, 256, 512] tokens\n",
        "- Overlap ratios: [0.1, 0.2, 0.3]  \n",
        "- Embedding models: [MiniLM, MPNet, BGE]\n",
        "- Retrieval k: [2, 3, 5]\n",
        "\n",
        "Track:\n",
        "- Precision/recall per topic cluster\n",
        "- Retrieval latency\n",
        "- Index size\n",
        "- Memory usage\n",
        "\n",
        "## User Feedback Integration\n",
        "\n",
        "Once you begin to get actual user feedback - combine with synthetic data.\n",
        "\n",
        "- Understand where synthetic data expands user feedback\n",
        "- Where does user feedback fit in gaps of synthetic data?\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def analyze_user_queries(queries):\n",
        "    # Cluster similar questions\n",
        "    embeddings = model.encode(queries)\n",
        "    clusters = KMeans(n_clusters=5).fit_predict(embeddings)\n",
        "    \n",
        "    # Analyze patterns within clusters\n",
        "    # Map to article's topic categories\n",
        "    return cluster_analysis\n",
        "```"
      ],
      "metadata": {
        "id": "_m6X1vcB1a9K"
      }
    }
  ]
}